{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "pd.options.display.max_rows=999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `reported_binary` Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc = pd.read_csv(\"fcc_values.csv\", parse_dates=[\"report_date\"])\n",
    "fcc = fcc.sort_values([\"ticker\", \"report_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reported_binary(x):\n",
    "    if x[\"reported\"] > x[\"consensus\"]:\n",
    "        return 1\n",
    "    elif x[\"reported\"] < x[\"consensus\"]:\n",
    "        return -1\n",
    "    elif x[\"reported\"] == x[\"consensus\"]:\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fcc[\"reported_binary\"] = fcc.apply(lambda x: reported_binary(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Overtime Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connection\n",
    "database = 'archive'\n",
    "username = 'quant'\n",
    "password = '8ad7a2be0e3ec3fbc3a16fab306f08d8'\n",
    "host     = '159.65.252.27'\n",
    "connection_string = 'postgresql://' + username + ':' + password + '@' + host + '/' + database\n",
    "engine   = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimum insert_date\n",
    "query = \"\"\"select min(insert_date) as start_date from scraping.raw_cm\"\"\"\n",
    "start_date = pd.read_sql_query(query, engine)\n",
    "start_date = pd.to_datetime(start_date[\"start_date\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data after we start scraping\n",
    "uncovered_data = fcc[fcc[\"report_date\"] < start_date]\n",
    "covered_data = fcc[(fcc[\"report_date\"] >= start_date) & (fcc[\"report_date\"] <= pd.Timestamp.today())]\n",
    "tickers = list(covered_data.ticker.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the target_name_map\n",
    "query = \"\"\"select * from configs.target_name_map where ticker in \"\"\" + str(tickers).replace(\"[\", \"(\").replace(\"]\", \")\")\n",
    "target_name_map = pd.read_sql_query(query, engine)\n",
    "target_name_map = target_name_map[[\"ticker\", \"cm_target_name\", \"cm_segment_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fiscal year and quarter\n",
    "query = \"\"\"\n",
    "select ticker, year, quarter, report_date \n",
    "from configs.quarter_name_map \n",
    "where insert_date = '2019-03-07'\n",
    "and ticker in \"\"\" + str(tickers).replace(\"[\", \"(\").replace(\"]\", \")\") + \"\"\"\n",
    "and report_date::date > '\"\"\" + str(start_date.date()) + \"' order by 1, 2\"\n",
    "quarter_name_map = pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge covered_data with target_name_map and quarter_name_map\n",
    "data = covered_data.merge(target_name_map, how='left').merge(quarter_name_map, how='left')\n",
    "data[\"start_date\"] = data.groupby(\"ticker\")[\"report_date\"].shift(1) + dt.timedelta(days=1)\n",
    "data[\"stop_date\"] = data[\"report_date\"]\n",
    "\n",
    "def update_start_date(x):\n",
    "    if (pd.isnull(x[\"start_date\"])) and (x[\"stop_date\"] > start_date):\n",
    "        return start_date\n",
    "    else:\n",
    "        return x[\"start_date\"]\n",
    "\n",
    "data[\"updated_start_date\"] = data.apply(lambda x: update_start_date(x), axis=1)\n",
    "del data[\"start_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the data base and append the data\n",
    "consensus_overtime = pd.DataFrame()\n",
    "for i in range(len(data)):\n",
    "    data_i = data.iloc[i]\n",
    "    if pd.isnull(data_i[\"updated_start_date\"]):\n",
    "        continue\n",
    "    else:\n",
    "        query = \"\"\"\n",
    "         select * from scraping.raw_cm \n",
    "         where insert_date::date >= '\"\"\" + str(data_i[\"updated_start_date\"].date()) + \"\"\"' \n",
    "         and insert_date::date <= '\"\"\" + str(data_i[\"stop_date\"].date()) + \"\"\"'\n",
    "         and ticker = '\"\"\" + data_i[\"ticker\"] + \"\"\"' \n",
    "         and metric_name = '\"\"\" + data_i[\"cm_target_name\"] + \"\"\"' \n",
    "         and segment_name = '\"\"\" + data_i[\"cm_segment_name\"] + \"\"\"' \n",
    "         and year = \"\"\" + str(int(data_i[\"year\"])) + \"\"\" \n",
    "         and quarter = \"\"\" + str(int(data_i[\"quarter\"])) + \"\"\" \n",
    "         order by 1\"\"\"\n",
    "        try:\n",
    "            consensus_i = pd.read_sql_query(query, engine)\n",
    "            consensus_i.rename(columns={\"reported\": \"consensus\"}, inplace=True)\n",
    "            consensus_i[\"report_date\"] = data_i[\"report_date\"]\n",
    "            consensus_i[\"reported\"] = data_i[\"reported\"]\n",
    "            consensus_i[\"reported_binary\"] = data_i[\"reported_binary\"]\n",
    "            consensus_overtime = consensus_overtime.append(consensus_i)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_overtime.rename(columns={\"insert_date\": \"consensus_date\"}, inplace=True)\n",
    "consensus_overtime = consensus_overtime[[\"ticker\", \"report_date\", \"consensus_date\", \n",
    "                                         \"consensus\", \"reported\", \"reported_binary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "uncovered_data[\"consensus_date\"] = np.nan\n",
    "final_data = uncovered_data.append(consensus_overtime).reset_index(drop=True)\n",
    "final_data = final_data[[\"ticker\", \"report_date\", \"consensus_date\", \n",
    "                         \"consensus\", \"reported\", \"reported_binary\"]].sort_values([\"ticker\", \"report_date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"fcc_with_consensus_overtime.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
