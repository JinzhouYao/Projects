{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cars.csv\", parse_dates=[\"date\", \"retrain_frequency\"])\n",
    "data = data.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if total_locations is inf or nan, equals to the last observed numbers by each ticker\n",
    "cars = data.copy()\n",
    "cars[\"total_locations\"] = cars[\"total_observations\"] / (cars[\"percent_of_locations_observed\"] / 100)\n",
    "cars[\"total_locations\"] = cars[\"total_locations\"].replace([np.inf, -np.inf], np.nan)\n",
    "cars[\"total_locations\"] = cars.groupby(\"ticker\")[\"total_locations\"].fillna(method='ffill')\n",
    "cars[\"total_locations\"] = cars[\"total_locations\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the data before 7 days before reporting\n",
    "reduced_cars = cars[cars[\"date\"] <= cars[\"retrain_frequency\"] - dt.timedelta(days=7)].reset_index(drop=True)\n",
    "col = list(reduced_cars.columns)\n",
    "new_col = col[:6] + [\"total_locations\"] + col[6:-1]\n",
    "reduced_cars = reduced_cars[new_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a helper dictionary\n",
    "# keys -> tickers\n",
    "# values -> report time of that ticker\n",
    "ticker_date = {}\n",
    "for tic in reduced_cars.ticker.unique():\n",
    "    dates = list(reduced_cars[reduced_cars[\"ticker\"] == tic].retrain_frequency.unique())\n",
    "    ticker_date[tic] = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dataframe\n",
    "daily = []\n",
    "for tic in ticker_date.keys():\n",
    "    for date in ticker_date[tic]:\n",
    "        df = reduced_cars.query(\"ticker == @tic and retrain_frequency == @date\")\n",
    "        vals = [tic, date, df.iloc[0][\"target\"], df.iloc[0][\"consensus\"]]\n",
    "        cols = [\"ticker\", \"date\", \"target\", \"consensus\"]\n",
    "        feature_names = list(df.columns[2:7])\n",
    "        for i in range(90):\n",
    "            try:\n",
    "                cols = cols + [\"day\" + str(i+1) + \"_\" + col for col in feature_names]\n",
    "                vals = vals + list(df.iloc[i][feature_names].values)\n",
    "            except:\n",
    "                vals = vals + [\"\"]*5\n",
    "        daily.append(vals)\n",
    "\n",
    "daily_df = pd.DataFrame(daily, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quarterly Features (Min, Max, Sum, Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_df_1 = daily_df[[\"ticker\", \"date\", \"consensus\", \"target\"]].copy()\n",
    "for col in reduced_cars.columns[2:7]:\n",
    "    df = pd.DataFrame()\n",
    "    df_sum = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).sum()[col].reset_index()\n",
    "    df_sum = df_sum.rename(columns={col: \"sum_\" + col, \"retrain_frequency\": \"date\"})\n",
    "    df_min = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).min()[col].reset_index()\n",
    "    df_min = df_min.rename(columns={col: \"min_\" + col, \"retrain_frequency\": \"date\"})\n",
    "    df_max = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).max()[col].reset_index()\n",
    "    df_max = df_max.rename(columns={col: \"max_\" + col, \"retrain_frequency\": \"date\"})\n",
    "    df_mean = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).mean()[col].reset_index()\n",
    "    df_mean = df_mean.rename(columns={col: \"mean_\" + col, \"retrain_frequency\": \"date\"})\n",
    "    df = df_sum.merge(df_min).merge(df_max).merge(df_mean)\n",
    "    quarterly_df_1 = quarterly_df_1.merge(df)\n",
    "    if col == \"observed\":\n",
    "        del quarterly_df_1[\"min_\" + col]\n",
    "        del quarterly_df_1[\"max_\" + col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quarterly Features (Kurtosis, Skew, Median, Standard Deviation, 25% Quantile, 75% Quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarterly_df_2 = daily_df[[\"ticker\", \"date\", \"consensus\", \"target\"]].copy()\n",
    "for col in reduced_cars.columns[2:7]:\n",
    "    if col != \"observed\":\n",
    "        df = pd.DataFrame()\n",
    "        df_mid = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).median()[col].reset_index()\n",
    "        df_mid = df_mid.rename(columns={col: \"mid_\" + col, \"retrain_frequency\": \"date\"})\n",
    "        df_kur = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).apply(pd.DataFrame.kurt)[col].reset_index()\n",
    "        df_kur = df_kur.rename(columns={col: \"kur_\" + col, \"retrain_frequency\": \"date\"})\n",
    "        df_ske = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).skew()[col].reset_index()\n",
    "        df_ske = df_ske.rename(columns={col: \"ske_\" + col, \"retrain_frequency\": \"date\"})\n",
    "        df_std = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).std()[col].reset_index()\n",
    "        df_std = df_std.rename(columns={col: \"std_\" + col, \"retrain_frequency\": \"date\"})\n",
    "        df_quant_25 = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).quantile(q=0.25)[col].reset_index()\n",
    "        df_quant_25 = df_quant_25.rename(columns={col: \"quant_25_\" + col, \"retrain_frequency\": \"date\"})\n",
    "        df_quant_75 = reduced_cars.groupby([\"ticker\", \"retrain_frequency\"]).quantile(q=0.75)[col].reset_index()\n",
    "        df_quant_75 = df_quant_75.rename(columns={col: \"quant_75_\" + col, \"retrain_frequency\": \"date\"})\n",
    "        df = df_mid.merge(df_kur).merge(df_ske).merge(df_std).merge(df_quant_25).merge(df_quant_75)\n",
    "        quarterly_df_2 = quarterly_df_2.merge(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily features\n",
    "daily_df.to_csv(\"daily_feature_with_cons.csv\", index=False)\n",
    "daily_df.drop([\"consensus\"], axis=1).to_csv(\"daily_feature_no_cons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all quarterly features\n",
    "quarterly_df = quarterly_df_1.merge(quarterly_df_2)\n",
    "quarterly_df.to_csv(\"quarterly_feature_with_cons.csv\", index=False)\n",
    "quarterly_df.drop([\"consensus\"], axis=1).to_csv(\"quarterly_feature_no_cons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quarterly features: sum, min, max, mean\n",
    "quarterly_df_1.to_csv(\"quarterly_feature_1_with_cons.csv\", index=False)\n",
    "quarterly_df_1.drop([\"consensus\"], axis=1).to_csv(\"quarterly_feature_1_no_cons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quarterly features: median, 25% quantile, 75% quantile, kurtosis, skew, standard deviation\n",
    "quarterly_df_2.to_csv(\"quarterly_feature_2_with_cons.csv\", index=False)\n",
    "quarterly_df_2.drop([\"consensus\"], axis=1).to_csv(\"quarterly_feature_2_no_cons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in results from xyme\n",
    "result_path = \"./results/\"\n",
    "q1_xgb_cons = pd.read_csv(result_path + \"josh_cars_q_1_xgb_cons.csv\", parse_dates=[\"date\"])[[\"ticker\", \"date\", \"preds\", \"truths\"]].sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "q2_xgb_cons = pd.read_csv(result_path + \"josh_cars_q_2_xgb_cons.csv\", parse_dates=[\"date\"])[[\"ticker\", \"date\", \"preds\", \"truths\"]].sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "q1_xgb_no_cons = pd.read_csv(result_path + \"josh_cars_q_1_xgb_no_cons.csv\", parse_dates=[\"date\"])[[\"ticker\", \"date\", \"preds\", \"truths\"]].sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "q2_xgb_no_cons = pd.read_csv(result_path + \"josh_cars_q_2_xgb_no_cons.csv\", parse_dates=[\"date\"])[[\"ticker\", \"date\", \"preds\", \"truths\"]].sort_values([\"ticker\", \"date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge consensus with results for further analysis\n",
    "consensus = quarterly_df[[\"ticker\", \"date\", \"consensus\"]].sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "q1_xgb_cons_reslt = q1_xgb_cons.merge(consensus, how=\"left\")\n",
    "q2_xgb_cons_reslt = q2_xgb_cons.merge(consensus, how=\"left\")\n",
    "q1_xgb_no_cons_reslt = q1_xgb_no_cons.merge(consensus, how=\"left\")\n",
    "q2_xgb_no_cons_reslt = q2_xgb_no_cons.merge(consensus, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional accuracy\n",
    "def dir_acc(x):\n",
    "    if np.sign(x[\"preds\"] - x[\"consensus\"]) == np.sign(x[\"truths\"] - x[\"consensus\"]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def err_acc(x):\n",
    "    if abs(x[\"preds\"] - x[\"truths\"]) < abs(x[\"consensus\"] - x[\"truths\"]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "for df in [q1_xgb_cons_reslt, q2_xgb_cons_reslt, q1_xgb_no_cons_reslt, q2_xgb_no_cons_reslt]:\n",
    "    df[\"dir_accuracy\"] = df.apply(lambda x: dir_acc(x), axis=1)\n",
    "    df[\"err_accuracy\"] = df.apply(lambda x: err_acc(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5490196078431373, 0.4803921568627451)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_1_xgb_cons\n",
    "q1_xgb_cons_reslt[\"dir_accuracy\"].mean(), q1_xgb_cons_reslt[\"err_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5882352941176471, 0.5098039215686274)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_1_xgb_no_cons\n",
    "q1_xgb_no_cons_reslt[\"dir_accuracy\"].mean(), q1_xgb_no_cons_reslt[\"err_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5686274509803921, 0.47058823529411764)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_2_xgb_cons\n",
    "q2_xgb_cons_reslt[\"dir_accuracy\"].mean(), q2_xgb_cons_reslt[\"err_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5294117647058824, 0.39215686274509803)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_2_xgb_no_cons\n",
    "q2_xgb_no_cons_reslt[\"dir_accuracy\"].mean(), q2_xgb_no_cons_reslt[\"err_accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
